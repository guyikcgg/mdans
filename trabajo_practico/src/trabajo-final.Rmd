---
title: 'Minería de Datos: Aprendizaje no supervisado y detección de anomalías'
author: "Cristian González Guerrero"
date: "7 September 2017"
lang: es
output: 
  pdf_document:
    pandoc_args: --latex-engine-opt=--shell-escape
    toc: true # table of content true
    toc_depth: 3  # upto three depths of headings (specified by #, ## and ###)
    number_sections: true  ## if you want number sections at each table header
header-includes:
   - \usepackage{svg}
bibliography: mdans-practic.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Presentación y visualización de los datos <a name="sec-visualization"></a>
## Descripción de los datos
El conjunto de datos `bankloan` puede ser abierto en R usando la siguiente instrucción.
```{r}
dataset = read.csv("data/bankloan-spss.csv", sep = ";", dec = ",")
```

Este conjunto de datos recoje 850 casos de clientes de un banco que piden préstamos,
incluyendo el hecho de que hayan incurrido en impago.
Se incluyen datos personales y salariales. También se incluyen datos estimados de probabilidad de impago.

En este conjunto de datos, las primeras 700 filas corresponden a clientes a los que
previamente se les ha dado un préstamo. Las últimas 150 filas se corresponden a
clientes que el banco necesita clasificar en función del riesgo de impago [@bankloan-dataset].

Como puede comprobarse con el siguiente código, el conjunto de datos presenta 850 filas y 12 columnas, lo que se corresponde a 850 observaciones de 12 variables.

```{r}
dim(dataset)
```

Puesto que se trata de un conjunto de datos exportado desde SPSS, será necesario dotarlo de la estructura pertinente. Esta estructura se ha consultado en [@bankloan-dataset], después de abrir el conjunto de datos del fichero `Loan_ROC.sav`.

```{r}
dataset$educ = factor(
  dataset$educ,
  labels = c(
    "Did not complete high school",
    "High school degree",
    "Some college",
    "College degree",
    "Post-undergraduate degree"
  )
)
dataset$impago = factor(
  dataset$impago,
  labels = c(
    "No",
    "Yes"
  )
)

str(dataset)
```


Ahora vemos que la mayor parte de las variables es de tipo numérico. En concreto, hay 4 variables enteras y 6 variables que toman valores continuos. Las 2 variables restantes son categóricas.

```{r}
data.types = sapply(dataset, class)
table(data.types)
```

A continuación se muestra un resumen de los datos contenidos en nuestro conjunto de datos.

```{r}
# Summary
summary(dataset)

# Data range
mt = matrix(nrow = ncol(dataset), ncol = 3)
mt[,1] = names(data.types)
mt[,2] = data.types
for (i in 1:ncol(dataset)) {
  if (data.types[i] == "numeric") {
    mt[i,3] = 
      sprintf("[%.2f, %.2f]", min(dataset[,i], na.rm = T), max(dataset[,i], na.rm = T))
  } else if (data.types[i] == "integer") {
    mt[i,3] = 
      sprintf("[%d, %d]", min(dataset[,i], na.rm = T), max(dataset[,i], na.rm = T))
  } else {
    mt[i,3] = 
      paste("{", paste(levels(dataset[,i]), collapse = ", "), "}", sep = "")
  }
}

mt
```

A continuación comprobamos si existen valores perdidos en nuestro conjunto de datos.

```{r}
data.frame(
  missing.values = colSums(is.na(dataset))
)
```

Podemos comprobar que los únicos valores perdidos son los 150 registros que deben clasificarse, siguiendo las instrucciones del enunciado.

## Visualización de los datos
A continuación se muestra un gráfico con las densidades de probabilidad de cada variable.

```{r}
library(reshape)
library(ggplot2)

# Useful functions
subset.numeric.dt = function(df) {
  return(
    df[,sapply(df, class) != "factor", drop = F]
  )
}
subset.numeric.cl = function(df) {
  return(
    df$impago
  )
}
subset.numeric    = function(df) {
  return(cbind(
    subset.numeric.dt(df),
    impago = df$impago
  ))
}
subset.factor = function(df) {
  return(
    df[,sapply(df, class) == "factor", drop = F]
  )
}

# Color blind palette
cbPalette = c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

# Plot numeric data
myData = melt.data.frame(
  subset.numeric.dt(dataset)
)
ggplot(myData) +
  geom_density(aes(value, fill = variable), alpha = 0.5) +
  facet_wrap(~variable, scales = "free") +
  ggtitle("Distribución de frecuencias de las variables numéricas")

# Plot categorical data
myData = melt.data.frame(
  subset.factor(dataset),
  id.vars = "impago"
)
ggplot(myData) +
  geom_bar(aes(value, fill = variable), alpha = 0.8) +
  coord_flip() +
  facet_wrap(~variable, scales = "free") +
  ggtitle("Distribución de frecuencias de las variables categóricas")
```


La variable impago es especialmente interesante, ya que el objetivo final de nuestro trabajo será tratar de averiguar qué clientes cometerán impago.

```{r}
# Plot numeric variables
myData = melt.data.frame(
  subset.numeric(dataset),
  id.vars = "impago"
)
ggplot(myData) +
  geom_freqpoly(aes(value, color = impago), bins = 20) +
  facet_wrap(~variable, scales = "free", ncol = 4) +
  ggtitle("Distribución de frecuencias de cada variable, en función del impago")

# Plot categorical variables
myData = melt.data.frame(
  subset.factor(dataset),
  id.vars = "impago"
)
ggplot(myData) +
  geom_bar(aes(value, fill = impago), position = "dodge") +
  coord_flip() +
  facet_wrap(~variable, scales = "free", ncol = 4) +
  ggtitle("Distribución de frecuencias de cada variable, en función del impago")

# Plot output variable frequencies
ggplot(dataset) +
  geom_bar(aes(impago, fill = impago)) +
  ggtitle("Frecuencias absolutas de la variable impago")
```

Como podemos observar, la mayor parte de los casos estudiados no tiene impagos. Sin embargo, una parte significativa (alrededor del 26%) produce impagos.

Habrá que buscar qué relación tienen estos impagos con el resto de variables, para ello aplicaremos técnicas de aprendizaje no supervisado.


# Clustering










# Detección de anomalías

# Reglas de asociación

## Visualización de las distintas clases

En primer lugar, visualizaremos mejor los datos con unos diagramas de cajas con bigotes.

```{r}
myData = melt.data.frame(
  subset.numeric(dataset),
  id.vars = "impago"
)
ggplot(myData) +
  geom_boxplot(aes(impago, value, fill = impago)) +
  facet_wrap(~variable, scales = "free") +
  ggtitle("Distribución de frecuencias de cada variable, en función del impago")
```

Estos gráficos demuestran que las variables `morapred1`, `morapred2` y `morapred3`, correspondientes a las predicciones de morosidad tienen una relación muy fuerte con el impago.

## Discretización

La aplicación de las reglas de asociación requiere que todas las variables de entrada sean discretas. Por esto aplicaremos una discretización antes de comenzar con las reglas de asociación.

Este paso es crucial, y los valores elegidos aquí serán determinantes para el proceso que viene a continuación. Normalmente, un experto debería indicarnos cuáles son los mejores puntos de corte para la discretización, o bien se debería llevar a cabo un proceso iterativo buscando los puntos de corte que nos lleven a reglas más interesantes. En este caso, aplicaremos un método automático para la obtención de los puntos de corte que optimicen la separación de las clases.

```{r}
library(discretization)
dataset = cbind(subset(dataset, select = -impago), dataset[,"impago",drop = F])
myDataset.numeric = subset.numeric(dataset)
myDataset.rm.na = myDataset.numeric[1:700, ]
cutp = disc.Topdown(myDataset.rm.na)$cutp


dataset.discretized = myDataset.rm.na
for (i in 1:(ncol(myDataset.rm.na)-1)) {
  dataset.discretized[,i] = cut(myDataset.rm.na[,i], cutp[[i]], include.lowest = T)
}
dataset.discretized = cbind(dataset[1:700,"educ",drop=F], dataset.discretized)
```


## Obtención de las transacciones

Una vez realizada la discretización del conjunto de datos, procedemos a la tranformación en transacciones.

```{r}
library(arules)
transactions = as(dataset.discretized, "transactions")
```

Ahora podemos ver un resumen de las transacciones.

```{r}
summary(transactions)
# image(transactions)
```

## Itemsets frecuentes

Es conveniente comprobar con qué frecuencia aparece cada item. De este modo, podemos ver los items más relevantes.

```{r}
itemFrequencyPlot(transactions, support = 0.1, cex.names = 0.8)
```

Como podemos observar, los items más frecuentes son `deudaotro=(-Inf,12.8]` y `ingresos=(15.5, Inf]`, con un soporte de aproximadamente 0.98.

Ahora podemos buscar qué itemsets son más frecuentes. Para ello, usaremos el método _apriori_.

```{r}
iBankloan = apriori(transactions, parameter = list(support = 0.1, target="frequent"))
iBankloan = sort(iBankloan, by = "support")
inspect(head(iBankloan, 20))

myData = data.frame(itemset.size = as.factor(size(iBankloan)))
ggplot(myData) + 
  geom_bar(aes(itemset.size), fill = cbPalette[1]) +
  ggtitle("Frecuencias absolutas de los tamaños de todos los itemsets")
```


## Itemsets maximales y cerrados
A la vista de esta gráfica, comprobamos que los itemsets con 5 y 6 elementos son muy frecuentes. A continuación se mostrarán los 5 itemsets maximales con mayor soporte.

```{r}
imaxBankloan = iBankloan[is.maximal(iBankloan)]
inspect(head(sort(imaxBankloan, by = "support"), 5))

length(imaxBankloan)
myData = data.frame(itemset.size = as.factor(size(imaxBankloan)))
ggplot(myData) + 
  geom_bar(aes(itemset.size), fill = cbPalette[3]) +
  ggtitle("Frecuencias absolutas de los tamaño de los itemsets maximales")
```

Como podemos comprobar, los itemsets maximales no tienen un soporte demasiado elevado. También podemos contemplar los itemsets cerrados.

```{r}
icloBankloan = iBankloan[is.closed(iBankloan)]
inspect(head(sort(icloBankloan, by = "support"), 5))

length(icloBankloan)
myData = data.frame(itemset.size = as.factor(size(icloBankloan)))
ggplot(myData) + 
  geom_bar(aes(itemset.size), fill = cbPalette[2]) +
  ggtitle("Frecuencias absolutas de los tamaño de los itemsets cerrados")
```

A continuación se muestra un conteo de los itemsets de cada tipo.

```{r}
myData = data.frame(
  frequent = length(iBankloan),
  closed = length(icloBankloan),
  maximal = length(imaxBankloan)
)
myData = melt(myData)
ggplot(myData) + 
  geom_col(aes(variable, value, fill = variable)) + 
  scale_fill_manual(values=cbPalette) +
  ggtitle("Frecuencias absolutas del número de itemsets de distintos tipos")
```

## Extracción de reglas
Para obtener las reglas de asociación, podemos seguir el método apriori en una primera aproximación. El uso de este método es posible gracias a que el conjunto de datos es bastante pequeño y nos podemos permitir el lujo de usar una técnica computacionalmente muy costosa. En nuestro caso, buscaremos reglas con un soporte mínimo del 10% y una confianza mínima del 80%.

```{r}
rules = apriori(
  transactions, 
  parameter = list(
    support    = 0.1,
    confidence = 0.8,
    minlen     = 2
  )
)

summary(rules)
```

A continuación, ordenaremos las reglas por confianza, para mostrar las que mejor resultado producen con respecto a esta métrica.

```{r}
rules.byConfidence = sort(rules, by = "confidence")

inspect(head(rules.byConfidence))
quality(head(rules.byConfidence))
```

También podemos ordenarlas por lift, o cualquier otra métrica.

```{r}
rules.byLift = sort(rules, by = "lift")

inspect(head(rules.byLift))
quality(head(rules.byLift))
```

# Estudio de las reglas deseadas
Vamos a observar las reglas cuyo lift sea superior a 2 o contengan información sobre el impago en el consecuente. Así podremos explorar reglas que nos lleven a predecir el impago, así como otras reglas interesantes.

```{r}
mySelectedRules = subset(
  rules, 
  subset = rhs %in% c(
    "impago=Yes",
    "impago=No"
  ) |
  lift > 2 
)

inspect(head(mySelectedRules))
```

Podemos eliminar las reglas redundantes haciendo lo siguiente:

```{r}
subsetMatrix = is.subset(mySelectedRules, mySelectedRules)
subsetMatrix[lower.tri(subsetMatrix, diag = T)] = NA
redundant = colSums(subsetMatrix, na.rm = T) >= 1
rules.pruned = mySelectedRules[!redundant]

inspect(head(rules.pruned))
```


## Medidas de interés adicionales
Llegados a este punto, podemos decir que hemos encontrado regla que parecen muy interesante, ya que nos permiten predecir la morosidad a partir de otro parámetro. Sin embargo, será necesario estudiar algunas medidas adicionales para comprobar cómo de interesante pueden ser esta y otras reglas.

```{r}
myInterestMeasures = interestMeasure(
  rules.pruned, 
  measure = c(
    "hyperConfidence", 
    "leverage",
    "phi",
    "gini"
  ),
  transactions = transactions
)

quality(rules.pruned) = cbind(
  quality(rules.pruned), 
  myInterestMeasures
)

inspect(head(sort(rules.pruned, by = "gini")))
```

## Visualización
Ahora podemos visualizar las reglas obtenidas con el paquete `arulesViz`. Existen varios tipos de visualización, que nos dan distintas perspectivas sobre las reglas encontradas.

```{r}
library(arulesViz)

plot(rules)

plot(rules.pruned[1:11], method="graph", control=list(type="items"))

# plot(rules.pruned, method="grouped")
# 
# plot(
#   rules.pruned[1:10],
#   method="paracoord",
#   control = list(reorder = TRUE)
# )
```

Como podemos comprobar en el gráfico relacional, existen relaciones muy interesantes que son autoexplicativas. En concreto, parece que los predictores de morosidad son bastante efectivos, ya que existe una relación con un amplio soporte entre que los predictores tengan un valor bajo y que no se hayan producido impagos. Asímismo, hay una fuerte relación (com poco soporte, pero mucho lift) entre un valor alto de `morapred1` y que se produzcan impagos. Otra regla que puede ser de interés  es que tener un valor alto en la variable `empleo` parece estar relacionado con no incurrir a impago.

# Bibliografía
