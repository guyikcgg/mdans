---
title: 'Minería de Datos: Aprendizaje no supervisado y detección de anomalías'
author: "Cristian González Guerrero"
date: "7 September 2017"
lang: es
output: 
  pdf_document:
    keep_tex: no
    number_sections: yes
    pandoc_args: --latex-engine-opt=--shell-escape
    toc: yes
    toc_depth: 3
header-includes:
   - \usepackage{svg}
bibliography: mdans-practic.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducción
Este es el trabajo práctico de la asignatura _Minería de Datos: Aprendizaje no Supervisado y Detección de Anomalías_, impartida en el Máster en Ciencia de Datos e Ingeniería de Computadores (DATCOM). En el mismo, se lleva a cabo un estudio sobre la base de datos _bankloan_, aplicando los métodos vistos en prácticas. Este trabajo, realizado de forma original por Cristian González Guerrero, se presenta para su valoración en la evaluación extraordinara de Septiembre de 2017.

# Presentación y visualización de los datos
## Descripción de los datos
El conjunto de datos `bankloan` puede ser abierto en R usando la siguiente instrucción.
```{r}
dataset = read.csv("data/bankloan-spss.csv", sep = ";", dec = ",")
```

Este conjunto de datos recoje 850 casos de clientes de un banco que piden préstamos,
incluyendo el hecho de que hayan incurrido en impago.
Se incluyen datos personales y salariales. También se incluyen datos estimados de probabilidad de impago.

En este conjunto de datos, las primeras 700 filas corresponden a clientes a los que
previamente se les ha dado un préstamo. Las últimas 150 filas se corresponden a
clientes que el banco necesita clasificar en función del riesgo de impago [@bankloan-dataset].

Como puede comprobarse con el siguiente código, el conjunto de datos presenta 850 filas y 12 columnas, lo que se corresponde a 850 observaciones de 12 variables.

```{r}
dim(dataset)
```

Puesto que se trata de un conjunto de datos exportado desde SPSS, será necesario dotarlo de la estructura pertinente. Esta estructura se ha consultado en [@bankloan-dataset], después de abrir el conjunto de datos del fichero `Loan_ROC.sav`.

```{r}
dataset$educ = factor(
  dataset$educ,
  labels = c(
    "Did not complete high school",
    "High school degree",
    "Some college",
    "College degree",
    "Post-undergraduate degree"
  )
)
dataset$impago = factor(
  dataset$impago,
  labels = c(
    "No",
    "Yes"
  )
)

str(dataset)
```


Ahora vemos que la mayor parte de las variables es de tipo numérico. En concreto, hay 4 variables enteras y 6 variables que toman valores continuos. Las 2 variables restantes son categóricas.

```{r}
data.types = sapply(dataset, class)
table(data.types)
```

A continuación se muestra un resumen de los datos contenidos en nuestro conjunto de datos.

```{r}
# Summary
summary(dataset)

# Data range
mt = matrix(nrow = ncol(dataset), ncol = 3)
mt[,1] = names(data.types)
mt[,2] = data.types
for (i in 1:ncol(dataset)) {
  if (data.types[i] == "numeric") {
    mt[i,3] = 
      sprintf("[%.2f, %.2f]", min(dataset[,i], na.rm = T), max(dataset[,i], na.rm = T))
  } else if (data.types[i] == "integer") {
    mt[i,3] = 
      sprintf("[%d, %d]", min(dataset[,i], na.rm = T), max(dataset[,i], na.rm = T))
  } else {
    mt[i,3] = 
      paste("{", paste(levels(dataset[,i]), collapse = ", "), "}", sep = "")
  }
}

mt
```

A continuación comprobamos si existen valores perdidos en nuestro conjunto de datos.

```{r}
data.frame(
  missing.values = colSums(is.na(dataset))
)
```

Podemos comprobar que los únicos valores perdidos son los 150 registros que deben clasificarse, siguiendo las instrucciones del enunciado.

## Visualización de los datos
A continuación se muestra un gráfico con las densidades de probabilidad de cada variable.

```{r}
library(reshape)
library(ggplot2)

# Useful functions
subset.numeric.dt = function(df) {
  return(
    df[,sapply(df, class) != "factor", drop = F]
  )
}
subset.numeric.cl = function(df) {
  return(
    df$impago
  )
}
subset.numeric    = function(df) {
  return(cbind(
    subset.numeric.dt(df),
    impago = df$impago
  ))
}
subset.factor = function(df) {
  return(
    df[,sapply(df, class) == "factor", drop = F]
  )
}

# Color blind palette
cbPalette = c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

# Plot numeric data
myData = melt.data.frame(
  subset.numeric.dt(dataset)
)
ggplot(myData) +
  geom_density(aes(value, fill = variable), alpha = 0.5) +
  facet_wrap(~variable, scales = "free") +
  ggtitle("Distribución de frecuencias de las variables numéricas")

# Plot categorical data
myData = melt.data.frame(
  subset.factor(dataset),
  id.vars = "impago"
)
ggplot(myData) +
  geom_bar(aes(value, fill = variable), alpha = 0.8) +
  coord_flip() +
  facet_wrap(~variable, scales = "free") +
  ggtitle("Distribución de frecuencias de las variables categóricas")
```


La variable impago es especialmente interesante, ya que el objetivo final de nuestro trabajo será tratar de averiguar qué clientes cometerán impago.

```{r}
# Plot numeric variables
myData = melt.data.frame(
  subset.numeric(dataset),
  id.vars = "impago"
)
ggplot(myData) +
  geom_freqpoly(aes(value, color = impago), bins = 20) +
  facet_wrap(~variable, scales = "free", ncol = 4) +
  ggtitle("Distribución de frecuencias de cada variable, en función del impago")

# Plot categorical variables
myData = melt.data.frame(
  subset.factor(dataset),
  id.vars = "impago"
)
ggplot(myData) +
  geom_bar(aes(value, fill = impago), position = "dodge") +
  coord_flip() +
  facet_wrap(~variable, scales = "free", ncol = 4) +
  ggtitle("Distribución de frecuencias de cada variable, en función del impago")

# Plot output variable frequencies
ggplot(dataset) +
  geom_bar(aes(impago, fill = impago)) +
  ggtitle("Frecuencias absolutas de la variable impago")
```

Como podemos observar, la mayor parte de los casos estudiados no tiene impagos. Sin embargo, una parte significativa (alrededor del 26%) produce impagos.

Habrá que buscar qué relación tienen estos impagos con el resto de variables, para ello aplicaremos técnicas de aprendizaje no supervisado.


# Clustering
Realizaremos varios ejercicios de clustering con el set de datos `bankloan`, probando distintas técnicas y comparando los resultados. Para ello, usaremos tres programas diferentes.


## KNIME
KNIME es software libre y puede usarse de forma totalmente libre y gratuita.

El primer ejercicio será instalar KNIME y familiarizarnos con esta herramienta, para lo que probaremos a crear un nuevo proyecto e insertar unos cuantos bloques, aprendiendo el flujo de trabajo del programa.

Una vez hecho esto, vamos a abrir nuestro conjunto de datos. Para ello será necesario un bloque del tipo `File Reader`, pues los datos proporcionados usan la *coma* como separador decimal. Una vez configurado este bloque, será conveniente normalizar los datos numéricos para que todas las columnas estén dentro de un rango razonable. Esto es fundamental en los problemas de clustering, donde siempre se miden distancias entre los puntos a etiquetar. En nuestro caso, nos hemos decantado por una normalización *Z-Score Normalization (Gaussian)*, para evitar posibles problemas por la presencia de anomalías.

A continuación, será necesario un bloque para clustering.

### K-Means
Comenzaremos usando un bloque `k-Means`, para usar dicho método en una primera aproximación.

También añadiremos algunos bloques para la visualización de los datos. En concreto, será necesario un bloque del tipo `Color Manager` para dar colores distintos a los diferentes clusters, así como un nodo `Scatter Plot` para realizar un diagrama de dispersión (o nube de puntos). De igual forma, añadiremos un bloque `Interactive Table` para facilitar el acceso a una tabla con los resultados del clustering.

Finalmente, comentar que hemos añadido algunos bloques más con diversos propósitos:

  * **`Numeric Binner`** con el objetivo de convertir la variable numérica `impago` en una variable de tipo categórica que tomara únicamente dos valores.
  * **`Row Filter`** con el objetivo de eliminar las filas con valores perdidos, que no son interesantes en la evaluación de los resultados.
  * **`String Replacer`** con el objetivo de hacer coincidir las etiquetas de los clusters generados con las etiquetas de la variable `impago`
  * **`Entropy Scorer`** con el objetivo de tener una medida de bondad del ajuste del modelo generado basada en la entropía.
  * **`Scorer`** con el objetivo de obtener una tabla de contingencia de la clasificación, así como las medidas que pueden derivarse de esta.

A continuación se presenta el diagrama de bloques generado finalmente.

\graphicspath{{img/knime/}}

\includegraphics[width=1\textwidth]{img/knime/cluster-scheme2-k-means.svg.png}

La configuración proporcionada al bloque de clustering es la siguiente:

  * number of clusters: 2
  * max. number of iterations: 99
  * included variables: *todas menos* `impago`

Para tener una aproximación gráfica de los resultados obtenidos por k-Means, tendremos que comparar gráficamente los datos originales con los resultados obtenidos.

La siguiente figura muestra los puntos del conjunto de datos original coloreados según la variable `impago`. En rojo, se muestran los clientes que no han incurrido en impago; en azul, los que han cometido impago; y en negro, los clientes que todavía no han recibido el crédito. En el eje $x$ se ha representado la variable `morapred1`, mientras que en la variable $y$ se ha representado `morapred2`. La elección de estas variables se debe a que tienen una relación muy fuerte con la variable `impago`.

\includegraphics[width=1\textwidth]{img/knime/cluster-scatterplot-before-k-means.svg.png}

A continuación se representan gráficamente los resultados obtenidos por el clustering. Los colores de los puntos han sido asignados en función del cluster.

\includegraphics[width=1\textwidth]{img/knime/cluster-scatterplot-after-k-means.svg.png}

Como puede observarse, los resultados obtenidos se distribuyen de una forma parecida a los datos iniciales, por lo que ésta parece una aproximación bastante buena. Podemos observar la calidad de la solución obtenida con los bloques `Entropy Scorer` y `Scorer`. A continuación se muestran los resultados medidos.



\includegraphics[width=1\textwidth]{img/knime/cluster-score-k-means.svg.png}


### DBSCAN
Probaremos ahora con otro tipo de clustering, como es DBSCAN.

A continuación se presenta el diagrama de bloques usado con la nueva configuración.

\includegraphics[width=1\textwidth]{img/knime/cluster-scheme2-dbscan.svg.png}

Tras probar distintos parámetros de configuración, los que han conseguido mejores resultados son:

  * distance selection: Euclidean
  * epsilon: 1.5
  * String Replacer:
    * Noise $\rightarrow$ Yes
    * cluster_? $\rightarrow$ No

A continuación se representan gráficamente los resultados obtenidos por el clustering con DBSCAN. Los distintos colores representan los clusters encontrados. En gris se representa el ruido. Los ejes son los mismos usados para las otras figuras.

\includegraphics[width=1\textwidth]{img/knime/cluster-scatterplot-after-dbscan.svg.png}

Al observar esta distribución parece que los resultados son bastante buenos. A continuación se muestran los resultados medidos con los bloques `Entropy Scorer` y `Scorer`.

\includegraphics[width=1\textwidth]{img/knime/cluster-quality-dbscan.svg.png}

\includegraphics[width=1\textwidth]{img/knime/cluster-score-dbscan.svg.png}


### k-Medoides
Vamos a trabajar ahora con otra aproximación. En este caso, usaremos k-Medoides.

A continuación se presenta el diagrama de bloques usado con la nueva configuración.

\includegraphics[width=1\textwidth]{img/knime/cluster-scheme2-k-medoids.svg.png}

Los parámetros de configuración ensayados han sido:

  * distance selection: Mahalanobis
  * distance variables: `morapred1`, `morapred2`, `morapred3`
  * $k$: 2

A continuación se representan gráficamente los resultados obtenidos por el clustering con k-Medoids. Cada cluster se ha rerpresentado de un color diferente. Los ejes son los mismos usados para las otras figuras.

\includegraphics[width=1\textwidth]{img/knime/cluster-scatterplot-after-k-medoids.svg.png}

Al observar esta distribución parece que no se ha conseguido encontrar un modelo que se ajustara correctamente a los datos. A continuación se muestran los resultados medidos con los bloques `Entropy Scorer` y `Scorer`.

\includegraphics[width=1\textwidth]{img/knime/cluster-quality-k-medoids.svg.png}

\includegraphics[width=1\textwidth]{img/knime/cluster-score-k-medoids.svg.png}


### Clustering jerárquico
A continuación hemos llevado a cabo clustering jerárquico, que es una aproximación completamente distinta de clustering.

A continuación se presenta el diagrama de bloques usado con la nueva configuración.

\includegraphics[width=1\textwidth]{img/knime/cluster-scheme2-hierarchical.svg.png}

Los parámetros de configuración que mejor resultado han conseguido son:

  * distance function: Euclidean
  * distance variables: `edad`, `empleo`, `direccion`, `ingresos`, `dedudaingr`, `morapred1`, `morapred2`, `morapred3`
  * linkage type: AVERAGE
  * number output cluster: 5
  
Estos parámetros han sido seleccionados observando el dendrograma y la distancia en función de k.

\includegraphics[width=1\textwidth]{img/knime/cluster-hierarchical1-dendrogram.svg.png}

\includegraphics[width=1\textwidth]{img/knime/cluster-hierarchical1-distance.svg.png}

A continuación se representan gráficamente los resultados obtenidos por el clustering jerárquico. Cada cluster se ha rerpresentado de un color diferente. Los ejes son los mismos usados para las otras figuras.

\includegraphics[width=1\textwidth]{img/knime/cluster-scatterplot-after-hierarchical.svg.png}

Al observar esta distribución parece que no se ha conseguido encontrar un modelo que se ajustara correctamente a los datos. A continuación se muestran los resultados medidos con los bloques `Entropy Scorer` y `Scorer`.

\includegraphics[width=1\textwidth]{img/knime/cluster-quality-hierarchical.svg.png}

\includegraphics[width=1\textwidth]{img/knime/cluster-score-hierarchical.svg.png}


### Clustering jerárquico con más de una función de distancia
Vamos a tratar de mejorar el clustering jerárquico realizado anteriormente con el uso de dos funciones de distancia distintas: una para los datos personales y otro para los datos económicos.

A continuación se presenta el diagrama de bloques usado con la nueva configuración.

\includegraphics[width=1\textwidth]{img/knime/cluster-scheme2-hierarchical2.svg.png}

Como puede comprobarse, se han usado bloques como `Column Splitter` y `Column Appender` para separar y volver a mezclar los atributos, `Create Bit Vector` y `Bit Vector Distances` para calcular distancias tipo bit, así como `Aggregated Distance` y `Hierarchical Clustering (DistMatrix)` para calcular la media de las distancias numérica y tipo bit y que la distancia resultante pueda ser usada en el proceso de clustering.

Los parámetros ensayados han sido:

  * numeric distance function: Euclidean
  * numeric variables: `empleo`, `ingresos`, `dedudaingr`, `dedudacred`, `dedudaotro`, `morapred1`, `morapred2`, `morapred3`
  * bit vector distance function: Tanimoto
  * bit vector variables: `edad`, `educ`, `direccion`
  * agregated distance: mean
  * linkage type: Single Linkage
  * number of clusters: 5
  
A continuación se muestran el dendrograma y la distancia en función de k.

\includegraphics[width=1\textwidth]{img/knime/cluster-hierarchical2-dendrogram.svg.png}

\includegraphics[width=1\textwidth]{img/knime/cluster-hierarchical2-distance.svg.png}

A continuación se representan gráficamente los resultados obtenidos por el clustering jerárquico. Cada cluster se ha rerpresentado de un color diferente. Los ejes son los mismos usados para las otras figuras.

\includegraphics[width=1\textwidth]{img/knime/cluster-scatterplot-after-hierarchical2.svg.png}



## PSPP
Puesto que SPSS es un software privativo con un coste inasequible para un estudiante o una pequeña empresa, se ha decidido realizar la práctica con un clon libre que forma parte del proyecto GNU: PSPP.

PSPP soporta el formato `.sav` de los ficheros creados en SPSS. Así, es posible visualizar el valor que toman las variables al igual que su descripción.

\includegraphics[width=1\textwidth]{img/pspp/PSPP-data-view.png}

\includegraphics[width=1\textwidth]{img/pspp/pspp-variable-view.png}

También es posible ver los comentarios del documento y de las variables.

\includegraphics[width=1\textwidth]{img/pspp/pspp-document-comments.png}

\includegraphics[width=1\textwidth]{img/pspp/pspp-variable-comments.png}

PSPP dispone de algunas opciones desde su interfaz gráfica, aunque es más potente desde la línea de comandos. Usando la GUI es posible realizar clustering usando el método de k-Means. Estos son los resultados obtenidos.

\includegraphics[width=1\textwidth]{img/pspp/pspp-cluster-results.png}



## R/RStudio
En esta sección trabajaremos los métodos tratados anteriormente a través de RStudio.

### Agrupamiento jerárquico
Comenzaremos con el agrupamiento jerárquico.

En primer lugar, será necesario quedarnos sólo con los datos numéricos, prescindiendo de la variable de clase. También habrá que cargar las bibliotecas que se van a usar para realizar los distintos cálculos y plots de clustering.

```{r jerarquico-normalizado_1}
library(cluster)
library(fpc)
dataset.numeric.dt = subset.numeric.dt(dataset)
dataset.numeric.cl = subset.numeric.cl(dataset)
```

A continuación, procederemos a calcular e clustering jerárquico, por el método de Ward, distancia euclídea. Este es el resultado obtenido:

```{r jerarquico-normalizado_2}
hc = hclust(dist(dataset.numeric.dt), method="ward.D2")
hc
```

Ahora podemos representar gráficamente los resultados con un dendrograma. Agruparemos los resultados en 5 clusters y exploraremos algunas medidas de bondad, como el coeficiente de silueta.

```{r jerarquico-normalizado_3}
plot(hc, labels = rep("", nrow(dataset.numeric.dt)))
rect.hclust(hc, k = 5)

# Agrupamiento
group = cutree(hc, k=5)

# Medidas de bondad de agrupamiento: coeficiente de silueta
plotcluster(dataset.numeric.dt, group)
shi = silhouette(group, dist(dataset.numeric.dt))
plot(shi, col = 1:2)

# Otras medidas de bondad
cluster.stats(
  dist(dataset.numeric.dt[1:700,]),
  group[1:700],
  alt.clustering = as.integer(dataset.numeric.cl[1:700])
)
```


### Agrupamiento por k-medias normalizadas
El uso de las k-medias requiere normalizar los atributos antes de ser aplicado.

```{r kmedias-normalizadas_1}
dataset.numeric.dt.norm = dataset.numeric.dt
for (j in 1:ncol(dataset.numeric.dt)) {
  x = dataset.numeric.dt[,j] 
  v = (x-mean(x)) / sqrt(var(x))
  dataset.numeric.dt.norm[,j] = v
}
```

Una vez normalizados los datos, podemos aplicar kmeans.

```{r kmedias-normalizadas_2}
kmeans.result = kmeans(dataset.numeric.dt.norm, 2)
kmeans.result

table(dataset.numeric.cl[1:700], kmeans.result$cluster[1:700])
```

Ahora podemos visualizar los resultados obtenidos con algunas gráficas.

```{r kmedias-normalizadas_3}
plot(
  dataset.numeric.dt.norm[,c("morapred1","morapred2")], 
  col = kmeans.result$cluster
)
points(
  kmeans.result$centers[,c("morapred1","morapred2")],
  col = 3:4,
  pch = 8,
  cex = 2
)
x = kmeans.result$cluster
plotcluster(dataset.numeric.dt.norm, x)
```

Al igual que hemos hecho con el clustering jerárquico, tendremos que realizar un análisis de bondad.

```{r kmedias-normalizadas_4}
shi = silhouette(kmeans.result$cluster, dist(dataset.numeric.dt.norm))
plot(shi, col=1:2)

group = kmeans.result$cluster
cluster.stats(
  dist(dataset.numeric.dt.norm[1:700,]),
  group[1:700],
  alt.clustering = as.integer(dataset.numeric.cl[1:700])
)
```


### Agrupamiento por k-medoides normalizados
A continuación realizaremos un agrupamiento por k-medoides con los datos que previamente habíamos normalizado.

```{r medoides-normalizado}
#Empezamos con k-medoides clasico (pam) con 2 clases. El gr?fico nos da tambien el coeficiente de silueta
pam.result = pam(dataset.numeric.dt.norm, 2)
table(dataset.numeric.cl[1:700], pam.result$cluster[1:700])

plot(pam.result)

group = pam.result$cluster
cluster.stats(
  dist(dataset.numeric.dt.norm[1:700,]),
  group[1:700],
  alt.clustering = as.integer(dataset.numeric.cl[1:700])
)
```


### Agrupamiento por k-medoides normalizados con valor óptimo de grupos
También podemos usar k-medoides usando el varlor óptimo de grupos ofrecido por la función.

```{r modoides-normalizado-valor-optimo-grupos}
#Pruebo con k-medoides optimo numero de grupos
pamk.result = pamk(dataset.numeric.dt.norm)
#pamk.result

pamk.result$pamobject$medoids
table(dataset.numeric.cl[1:700], pamk.result$pamobject$clustering[1:700])

plot(pamk.result$pamobject)

identical(pamk.result$pamobject$clustering, pam.result$cluster)
```

Como podemos observar, el valor óptimo de clusters encontrado es 2, por lo que los resultados van a ser idénticos a los obtenidos anteriormente con k-medoides.

### Agrupamiento con DBSCAN
Lo siguiente que haremos será probar un agrupamiento por DBSCAN.

```{r dbscan}
library(fpc)

ds = dbscan(dataset.numeric.dt.norm, eps = 1.5, MinPts = 5)
table(ds$cluster[1:700], dataset.numeric.cl[1:700])

plot(ds, dataset.numeric.dt.norm[c("morapred1", "morapred2")])

x = table(ds$cluster[1:700], dataset.numeric.cl[1:700])
nc = length(x[,1])
nc
shi = silhouette(ds$cluster, dist(dataset.numeric.dt.norm))
plot(shi, col = 1:nc)
```

Como podemos comprobar, DBSCAN ha encontrado dos clusters diferentes. Como pasaba en las pruebas con KNIME, parece que el ruido está asociado a la clase impago, mientras que el resto de clusters están asociados a la otra clase.

### Agrupamiento con k-means difusas normalizado
Seguidamente, trabajaremos con un modelo basado en k-medias difusas.

```{r fuzzymeans-normalizado}
fuzzy.result = fanny(dataset.numeric.dt.norm, 2, memb.exp = 1.4)

str(fuzzy.result)
head(fuzzy.result$membership, 20)

table(fuzzy.result$clustering[1:700], dataset.numeric.cl[1:700])
plot(fuzzy.result)

group = fuzzy.result$clustering
cluster.stats(
  dist(dataset.numeric.dt.norm[1:700,]),
  group[1:700],
  alt.clustering = as.integer(dataset.numeric.cl[1:700])
)
```

Como podemos ver, ahora en vez de un cluster, se tiene un grado de pertenencia a cada cluster. Esto es útil, ya que permite ajustar el umbral y calcular momentos estadísticos interesantes como el *Area Under the Curve* (AUC) de la curva ROC.

### Agrupamiento jerárquico con varias funciones de distancia
A continuación usaremos dos funciones de distancia distintas: una para datos personales y otra para datos de situación económica. Empezaremos con el agrupamiento jerárquico.

El primer paso será separar los datos personales de los datos de situación económica en dos *data frames* diferentes.

```{r distancias-jerarquico_1}
dataset.numeric.dt.norm.personal = data.frame(
  subset(
    dataset.numeric.dt.norm,
    select = c(edad, direccion)
  ),
  educ = as.integer(dataset$educ)
)
dataset.numeric.dt.norm.economic = subset(
  dataset.numeric.dt.norm,
  select = c(empleo, ingresos, deudaingr, deudacred, deudaotro)
)
```

A continuación, calcularemos las distancias euclída y binaria entre las variables ya seaparadas. Tendremos que calcular la distancia final como la media entre estas dos distancias.

```{r distancias-jerarquico_2}
# Calculamos distancias distintas
dist1 = dist(dataset.numeric.dt.norm.economic)
dist2 = dist(dataset.numeric.dt.norm.personal, method="binary")

# Calculamos distancia ponderada y Cluster
dista = (dist1+dist2)/2
```

Una vez hecho esto, podremos proceder al clustering, como hicimos anteriormente.

```{r distancias-jerarquico_3}
h = hclust(dista, method="ward.D2")
h
plot(h, labels = rep("", length(h$labels)))
rect.hclust(h, k = 5)
grupo = cutree(h, k=5)
```

De nuevo, practicaremos un análisis de bondad.

```{r distancias-jerarquico_4}
# Análsis de bondad, usamos solo 150 valores para dibujar

idx = sample(1:700, 150)

plotcluster(dataset.numeric.dt.norm.personal[idx,], grupo[idx])
plotcluster(dataset.numeric.dt.norm.economic[idx,], grupo[idx])

d1 = dist(dataset.numeric.dt.norm.personal[idx,])
d2 = dist(dataset.numeric.dt.norm.economic[idx,])
d3 = (d1+d2)/2

shi = silhouette(grupo[idx], d3)
plot(shi, col=1:5)

cluster.stats(dista, grupo)
```

Como podemos observar, los datos económicos nos proporcionan más información sobre la posibilidad de incurrir en impago que los datos personales.

### Agrupamiento por k-medias con distintas funciones de distancia
Podemos repetir el procedimiento usado para medir distancias distintas con las particiones jerárquicas y aplicar ahora k-medias.

```{r distancias-k-medias_1}
kmeans.result=kmeans(dista, 2)

grupo = kmeans.result$cluster
```

De nuevo, realizaremos un anáiliss de bondad sobre los resultados obtenidos

```{r distancias-k-medias_2}
idx = sample(1:700, 250)

plotcluster(dataset.numeric.dt.norm.personal[idx,], grupo[idx])
plotcluster(dataset.numeric.dt.norm.economic[idx,], grupo[idx])

d1 = dist(dataset.numeric.dt.norm.personal[idx,])
d2 = dist(dataset.numeric.dt.norm.economic[idx,])
d3 = (d1+d2)/2

shi = silhouette(grupo[idx],d3)
plot(shi, col=1:5)

cluster.stats(dista,grupo)
```


### Agrupamiento por k-medoides con distintas funciones de distancia
Realizaremos el mismo proceso ahora, pero esta vez usando k-medoides.

```{r distancias-k-medoides}
pam.result = pam(dista, 2)

grupo = pam.result$clustering

plotcluster(dataset.numeric.dt.norm.personal[idx,],grupo[idx])
plotcluster(dataset.numeric.dt.norm.economic[idx,],grupo[idx])

shi = silhouette(grupo[idx],d3)
plot(shi,col=1:5)

cluster.stats(d3,grupo[idx])
```

Como podemos comprobar, cada uno de los métodos produce un agrupamiento distinto. Esto hace que el problema de agrupamiento tenga múltiples soluciones y que sea necesario un estudio minucioso de las relaciones entre las distintas variables para elegir el modelo que mejor se ajuste. Como primera aproximación, pueden tomarse las medidas de bondad obtenidas para comparar qué método es mejor siguiendo alguna de ellas.



# Detección de anomalías

## Carga de bibliotecas y funciones
Para realizar las tareas de detección de anomalías, haremos uso de las bibliotecas instaladas en prácticas, así como de las funciones proporcionadas. Podemos cargar estas bibliotecas y funciones en el *workspace* del siguiente modo.

```{r anomalias-funciones-bibliotecas}
source("anomalias/Outliers_A2_Librerias.R")
source("anomalias/Outliers_A3_Funciones.R")
```

Una vez hecho esto, procederemos a estudiar los diferentes métodos de detección de anomalías. Comenzaremos por la detección de anomalías por métodos sencillos en una variable, pasaremos luego a la detección en varias variables y finalmente usaremos métodos basados en distancia y en clustering.

## Detección de outliers en una variable mediante el rango intercuartílico (IQR)
Comenzaremos por la detección de anomalías usando el método más sencillo posible: el rango intercuartílico. Para ello, empezaremos preparando los datos a procesar. Comenzaremos buscando los outliers en una única columna de nuestros datos numéricos. Más tarde, procederemos a hacer esto con el resto de columnas.

```{r}
mydata.numeric  = dataset.numeric.dt
indice.columna  = 4
nombre.mydata   = "bankloan"

mydata.numeric.scaled = scale(mydata.numeric)
columna         = mydata.numeric[, indice.columna]
nombre.columna  = names(mydata.numeric)[indice.columna]
columna.scaled  = mydata.numeric.scaled[, indice.columna]
```

### Cómputo de los outliers IQR
A continuación, calculamos los cuartiles de la distribución que representan los datos de la columna.


```{r}
cuartil.primero = quantile(columna, 0.25)
cuartil.tercero = quantile(columna, 0.75)
iqr = cuartil.tercero-cuartil.primero
```

Un dato será considerado outlier si se encuentra muy alejado del centro de la distribución de frecuencias. En concreto, consideraremos que un dato es un outlier normal si su valor es superior al del tercer cuartil más 1.5 veces el rango intercuartílico, o, equivalentemente, si está por debajo del primer cuartil menos 1.5 veces el rango intercuartílico. Consideraremos que un dato es un outlier extremo si el dato está alejado más de tres veces el rango intercuartílico.

Para hacer esto, calcularemos una serie de umbrales y compararemos cada dato con los mismos. El resultado es el que sigue.

```{r}
extremo.superior.outlier.normal  = (iqr * 1.5) + cuartil.tercero
extremo.inferior.outlier.normal  = cuartil.primero - (iqr * 1.5)
extremo.superior.outlier.extremo = (iqr * 3) + cuartil.tercero
extremo.inferior.outlier.extremo = cuartil.primero - (iqr * 3)

vector.es.outlier.normal  = 
  columna > extremo.superior.outlier.normal |
  columna < extremo.inferior.outlier.normal
print(paste("Total de outliers normales:", sum(vector.es.outlier.normal)))

vector.es.outlier.extremo = 
  columna > extremo.superior.outlier.extremo |
  columna < extremo.inferior.outlier.extremo   
print(paste("Total de outliers extremos:", sum(vector.es.outlier.extremo)))
```

A continuación, comprobaremos en qué filas se encuentran los outliers detectados, y qué valor toman dichos outliers.

```{r}
# Outliers normales
claves.outliers.normales = which(vector.es.outlier.normal)
data.frame.outliers.normales = mydata.numeric[vector.es.outlier.normal,]    

print(paste0("Outliers normales en la columna '", nombre.columna, "':"))
print(data.frame.outliers.normales)

# Outliers extremos
claves.outliers.extremos = which(vector.es.outlier.extremo)
data.frame.outliers.extremo = mydata.numeric[vector.es.outlier.extremo,]

print(paste0("Outliers extremos en la columna '", nombre.columna, "':"))
print(data.frame.outliers.extremo)
```

Tembién es interesante conocer la desviación de cada outlier con respecto a la media de la columna. Para ello usaremos la variable normalizada.

```{r}
valores.normalizados.outliers.normales = columna.scaled[vector.es.outlier.normal]
valores.normalizados.outliers.normales
```

Esta información puede resumirse con algunos gráficos. A continuación representaremos los valores que toma la columna estudiada en un gráfico, señalando en rojo los outliers. Para ello, haremos uso de las funciones proporcionadas.

```{r}
MiPlot_Univariate_Outliers(columna, claves.outliers.normales, paste(nombre.columna, "> outliers normales"))
MiPlot_Univariate_Outliers(columna, claves.outliers.extremos, paste(nombre.columna, "> outliers extremos"))
```

Otra posible representación es el diagrama de caja con bigotes. En este caso, representaremos los outliers como puntos rojos en el diagrama. Tener el diagrama con los datos escalados es útil para poder comparar distintas distribuciones.

```{r}
MiBoxPlot_IQR_Univariate_Outliers(mydata.numeric, indice.columna)
MiBoxPlot_IQR_Univariate_Outliers(mydata.numeric.scaled, indice.columna)
```

En los archivos proporcionados en las prácticas, se incluyen funciones para el cálculo de outliers con este método. Estas funciones son de gran utilidad, ya que permiten la automatización de los cálculos expuestos anteriormente. Además, gracias a estas funciones podremos calcular los outliers en varias columnas con pocas instrucciones.

```{r}
# Cálculo de outliers en cada columna. 
# El resultado es un dataframe del mismo tamaño que nuestro dataset,
# cuyas columnas se corresponden a los valores calculados anteriormente
# en la variable `vector.es.outlier.normal`
frame.es.outlier = as.data.frame(sapply(
  1:ncol(mydata.numeric),
  function(x) vector_es_outlier_IQR(mydata.numeric, x)
))
names(frame.es.outlier) = names(mydata.numeric)
head(frame.es.outlier, 10)

print("Número total de outliers por columna:")
print(apply(frame.es.outlier, 2, sum))

indices.de.outliers.en.alguna.columna.como.una.lista = 
  sapply(
    1:ncol(mydata.numeric), 
    function(x) {
      vector_claves_outliers_IQR(mydata.numeric, x)
    }
  )
indices.de.outliers.en.alguna.columna =
  unlist(indices.de.outliers.en.alguna.columna.como.una.lista)

print("Filas en las que hay outliers:")
indices.de.outliers.en.alguna.columna
```


A continuación podemos observar la distancia de los outliers a la media, pero mostrando a la vez todas las columnas. Sólo las posiciones en las que se hayan encontrado outliers presentarán valores extremos.

```{r}
head(mydata.numeric.scaled[indices.de.outliers.en.alguna.columna , ], 10)
```

Ahora podemos visualizar los diagramas de caja con bigotes de cada variable normalizada, representando en rojo los outliers de cada columna.

```{r}
MiBoxPlot_juntos (mydata.numeric) + coord_flip()
```


### Detección de outliers en una variable usando tests estadísticos
Otra forma de atajar el problema de la detección de outlies en una variable es usando tests estadísticos.


#### Test de Grubbs
Este test nos permite comprobar si existe un único outlier en una única variable, asumiendo que ésta sigue una distribución normal. Habrá que tener precaución con el uso de este test, ya que la asunción de normalidad puede hacer que produzca resultados extraños con otras distribuciones. Además, el test puede indicar que no existe un outlier si efectivamente hay más de uno.

A continuación, aplicaremos el test de Grubbs sobre la columna estudiada anteriormente (ingresos).

```{r}
columna = mydata.numeric$ingresos

test.de.Grubbs = grubbs.test(columna, two.sided = TRUE)
test.de.Grubbs

alpha = 0.025 

if (test.de.Grubbs$p.value < alpha){
  indice.de.outlier.Grubbs = 
    order(abs(columna - mean(columna)), decreasing = T)[1]
  valor.de.outlier.Grubbs  = 
    columna[indice.de.outlier.Grubbs]
}

print("Índice del outlier encontrado:")
indice.de.outlier.Grubbs
print("Valor del outlier encontrado:")
valor.de.outlier.Grubbs


MiPlot_Univariate_Outliers(
  columna, 
  indice.de.outlier.Grubbs,
  "Test de Grubbs sobre la columna 'ingresos'"
)
```

Como podemos observar, el test de Grubbs ha arrojado un resultado positivo, dado que el p-value está muy por debajo de los umbrales usuales. Esto verifica que existe un outlier en nuestra columna. Además, hemos podido identificarlo y mostrar cuál es.

Este proceso podría repetirse do forma iterativa, detectando cada vez un outlier y eliminándolo. Sin embargo, existe un caso en el que el outlier no es detectado por estar enmascarado por otro dato, que también debería ser considerado un outlier. En este caso, el test falla y no puede afirmarse con confianza que exista un outlier.

```{r}
columna = c(45,56,54,34,32,45,67,45,67,154,125,65)

test.de.Grubbs = grubbs.test(columna, two.sided = TRUE)
test.de.Grubbs$p.value

if (test.de.Grubbs$p.value < alpha){
  indice.de.outlier.Grubbs = 
    order(abs(columna - mean(columna)), decreasing = T)[1]
  valor.de.outlier.Grubbs  = 
    columna[indice.de.outlier.Grubbs]
}

MiPlot_Univariate_Outliers(
  columna, 
  indice.de.outlier.Grubbs,
  "Test de Grubbs con outlier enmascarado"
)
```


#### Test de Rosner
El test de Rosner nos permite comprobar si existen $k$ o menos outliers en una variable, asumiendo normalidad. Esto puede solventar el problema visto anteriormente con el test de Grubbs.

Para comprobarlo, aplicaremos el test de Rosner con $k=4$ a los datos usados anteriormente, en los que el test de Grubbs no encontraba los outliers que se enmascaraban. El test ordena las distancias de cada punto a la media de mayor a menor y lanza el test de hipótesis para comprobar si hay menos de $k=4$ outliers.

```{r}
test.de.rosner = rosnerTest(columna, k=4)

is.outlier.rosner = test.de.rosner$all.stats$Outlier
k.mayores.desviaciones.de.la.media = test.de.rosner$all.stats$Obs.Num
indices.de.outliers.rosner = k.mayores.desviaciones.de.la.media[is.outlier.rosner]
valores.de.outliers.rosner = columna[indices.de.outliers.rosner]

print("Índices de las k-mayores desviaciones de la media")
k.mayores.desviaciones.de.la.media
print("De los k valores fijados, ¿Quién es outlier?")
is.outlier.rosner 
print("Los índices de los outliers son:")
indices.de.outliers.rosner
print("Los valores de los outliers son:")
valores.de.outliers.rosner

MiPlot_Univariate_Outliers(
  columna,
  indices.de.outliers.rosner,
  "Test de Rosner"
)
```

Como podemos observar, el test de Rosner ha detectado correctamente los dos outliers presentes en los datos. A continuación observaremos el comportamiento de este test con la columna de datos inicial. Para ello usaremos la función `MiPlot_resultados_TestRosner` proporcionada en las prácticas.

```{r}
columna = mydata.numeric$ingresos

MiPlot_resultados_TestRosner(columna)
```

Como podemos observar, este test da positivo para los cuatro datos más alejados de la media, indicando que son outliers.

## Detección de outliers multivariantes
La detección de outliers en múltiples variables trata de detectar puntos que, a pesar de mostrar valores no demasiado alejados de la media en cada una de las variables, presentan una combinación anómala de valores. Una forma de averiguar este hecho es usando la distancia de Mahalanobis.

### Paquete `mvoutlier`
A continuación se muestra cómo pueden obtenerse los outliers multivariantes usando el paquete `mvoutlier`. Cabe destacar que no es necesaria la normalización de los datos, ya que la distancia de Mahalanobis es robusta ante los cambios de escala.

```{r}
# Calculamos los valores de significancia
alpha.value = 0.05
alpha.value.penalizado = 
  1 - ( 1 - alpha.value) ^ (1 / nrow(mydata.numeric))

# Establecemos la semilla para el método iterativo que calcula 
# MCD, con el fin de obtener siempre los mismos resultados
set.seed(1)

# Generamos el gráfico con los outliers encontrados.
mvoutlier.plot = uni.plot(
  mydata.numeric, symb = FALSE,
  alpha = alpha.value.penalizado
)
```

A pesar de representarse varias columnas, cada una correspondiente a un atributo, los outliers han sido obtenidos teniendo en cuenta las distintas variables. Este es el motivo por el que se han detectado outliers que se encuentran muy cerca de la media en algunas de las variables.

A continuación vamos a analizar qué variables son las más influyentes en la designación de los outliers. Para eso, tendremos en cuenta el valor normalizado de cada variable, para ver cuánto se desvía de la media. También representaremos los valores de las variables gráficamente, con un biplot sobre las componentes principales. De esta forma podremos observar las interacciones entre las distintas variables.

Comenzaremos por construir un vector que nos indique cada fila que se ha detectado como outlier.

```{r}
# Construímos la variable is.MCD.outlier
is.MCD.outlier = mvoutlier.plot$outliers

numero.de.outliers.MCD = sum(is.MCD.outlier)
print(paste("Número de outliers MCD:", numero.de.outliers.MCD))
```

A continuación comprobaremos el valor normalizado de cada outlier, es decir, cuánto se desvía de la media de cada columna. De esta forma, tendremos un valor numérico para la desviación mostrada en la gráfica generada con `uni.plot`.

```{r}
data.frame.solo.outliers = mydata.numeric.scaled[is.MCD.outlier, ]
head(data.frame.solo.outliers, 10)
```

Seguidamente, representamos las cajas con bigotes de las distintas distribuciones con etiquetas para los outliers. Para conseguir visualizar la gráfica correctamente, sólo representaremos los diez primeros puntos.

```{r}
MiBoxPlot_juntos(
  mydata.numeric[1:10,], 
  is.MCD.outlier
) + coord_flip()
```

En esta gráfica podemos ver que muchos de los valores detectados como outliers lo son en una única variable, ya que están muy alejados de la distribución central en alguna de las variables. Esto se ve muy claro con los datos etiquetados como _1_ y _7_. En cambio, hay otros datos que no se encuentran el los bordes de las distribuciones, pero han sido detectados como outliers MCD. Esto se ve claramente con el dato etiquetado como _2_.

El BiPlot también nos mostrará esta información, junto con las correlaciones entre las variables. Los puntos mostrados son proyecciones de $n$ dimensiones en el plano, por lo que se trata de una representación aproximada.

```{r}
MiBiPlot_Multivariate_Outliers(
  mydata.numeric[1:150,], 
  is.MCD.outlier[1:150], 
  "BiPlot de MCD"
)
```

El BiPlot muestra claramente que el dato _2_ no es un outlier univariate en ninguna variable, pues está muy cerca del origen de los ejes.

A continuación vamos a construir una matriz con los gráficos de dispersión obtenidos al cruzar todas las variables. En rojo, se representará el dato _2_.

```{r}
indices.de.interes = 2
MiPlot_Univariate_Outliers(
  mydata.numeric[1:50,], 
  indices.de.interes, 
  "Scatterplots de MCD (primeros 50 datos)"
)
```

Puede apreciarse que no hay una combinación clara de dos variables que hagan que el dato _2_ sea un outlier. Por tanto, deben estar interviniendo más de dos variables.

### Paquete `CerioliOutlierDetection`
A continuación se repetirán los cómputos realizados anteriormente con el paqute `mvoutlier`, pero esta vez se usará el paquete `CerioliOutlierDetection`. Éste calcula los outliers calculando las distancias de Mahalanobis y usando la aproximación de Hardin/Rocke. La estimación de la matriz de covarianzas es la estimación robusta según MCD.

```{r}
is.Cerioli.outlier = cerioli2010.irmcd.test(
  mydata.numeric, 
  signif.gamma = alpha.value
)$outliers

indices.outliers.cerioli = which(is.Cerioli.outlier)
numero.de.outliers.cerioli = sum(is.Cerioli.outlier)

print("Número de outlies 'Cerioli':")
numero.de.outliers.cerioli 
print("Número de outlies 'mvoutlier':")
numero.de.outliers.MCD
```

Como podemos comprobar, este método ha detectado menos outliers que el anterior. Sin embargo, el punto _2_ sigue apareciendo como outlier, por lo que podremos considerar que se trata, efectivamente, de un outlier multivariante.

```{r}
MiBiPlot_Multivariate_Outliers(
  mydata.numeric[1:150,], 
  is.Cerioli.outlier[1:150], 
  "MCD outliers según el paquete Cerioli"
)
```

## LOF
Vamos a explorar otros métodos de detección de anomalías. En este caso, vamos a usar la medida de distancia LOF para decidir si un punto es o no un outlier. Dado que este método se basa en una distancia, será necesario trabajar con los datos normalizados.

Cabría cuestionarse si la distancia de Mahalanobis es adecuada para detectar outliers en el caso que nos ocupa. Para ello, podemos usar la función `corr.plot`.

```{r}
mis.datos.numericos  = dataset.numeric.dt
mis.datos.numericos.normalizados    = dataset.numeric.dt.norm

reset_par()
corr.plot(mis.datos.numericos[,7], mis.datos.numericos[,8])
```

El gráfico nos muestra un diagrama de dispersión al cruzar las variables 1 y 4. Vemos que, a pesar de haber un único grupo, muchos de los puntos quedan fuera de la elipse roja, que representa la distancia de Mahalanobis construida con la estimación robusta de la matriz de covarianzas y las corrrespondientes medias a partir del grupo más numeroso. El hecho de que queden tantos puntos fuera de esta elipse hará que se encuentren muchos outliers, haciendo tal vez que se detecten puntos que no son realmente outliers.

A continuación usaremos la métrica LOF teniendo en cuenta 5 vecinos y representaremos gráficamente los resultados.

```{r}
numero.de.vecinos.lof = 5

lof.scores = lofactor(
  mis.datos.numericos.normalizados,
  k=numero.de.vecinos.lof
)

plot(lof.scores)
```

Como podemos ver, hay algunos puntos que tienen un valor de LOF bastante alto. Podríamos considerar que los puntos con un LOF por encima de 1.4 serán outliers.

```{r}
is.lof.outlier = lof.scores > 1.4
 
MiBiPlot_Multivariate_Outliers(
  mis.datos.numericos[1:150,],
  is.lof.outlier[1:150],
  "LOF Outliers"
)
```

Como podemos apreciar, se han encontrado menos outliers esta vez, y los resultados son bastante distintos a los obtenidos con las técnicas anteriores. La elección de la técnica a usar deberá basarse en la distribución de las variables.

Otra posible aproximación para seleccionar los outliers basándonos en LOF sería ordenar los datos por LOF y decidir nosotros el número de outliers que hay en nuestro dataset.


## Detección de outliers basada en clustering
Otra posible aproximación en la detección de anomalías es usar clustering. Este tipo de aproximaciones también requieren los datos normalizados.

### Clustering con k-means
A continuación trataremos de buscar 10 outliers usando 3 clusters usando k-means con una distancia euclídea.

```{r}
numero.de.outliers = 10
numero.de.clusters = 3

# Hacemos que los resultados de k-means sean repetibles
set.seed(2)

# Construimos el modelo
modelo.kmeans = kmeans(
  mis.datos.numericos.normalizados,
  centers = numero.de.clusters
)
# ¿A qué cluster pertenece cada dato?
indices.clustering.bankloan = modelo.kmeans$cluster
# ¿Cuáles son los centros de los clusters obtenidos?
centroides.normalizados.bankloan  = modelo.kmeans$centers
```

A continuación calcularemos la distancia de cada punto a su centroide, usando la función `distancias_a_centroides`. Cuanto más alejado esté un dato del centroide de su cluster, más probable es que se trate de un outlier.

```{r}
distancias_a_centroides = function(
  datos.normalizados, 
  indices.asignacion.clustering, 
  datos.centroides.normalizados
) {
  sqrt(
    rowSums(
      (
        datos.normalizados - 
         datos.centroides.normalizados[indices.asignacion.clustering,]
      )^2
    )
  )
}

# Calculamos las distancias
dist.centroides.bankloan = 
  distancias_a_centroides (
    mis.datos.numericos.normalizados, 
    indices.clustering.bankloan, 
    centroides.normalizados.bankloan
  )

indices.outliers.bankloan = 
  order(dist.centroides.bankloan, decreasing=T)[1:numero.de.outliers]
```

Vamos a definir una función para realizar las tareas anteriores.

```{r}
# Devuelve los índices de los top-k clustering outliers y sus distancias a los centroides
top_clustering_outliers = function(datos.normalizados, 
                                   indices.asignacion.clustering, 
                                   datos.centroides.normalizados, 
                                   numero.de.outliers){
  
  dist_centroides = distancias_a_centroides (datos.normalizados, 
                                             indices.asignacion.clustering, 
                                             datos.centroides.normalizados)
  
  indices = order(dist_centroides, decreasing=T)[1:numero.de.outliers]
  
  list(distancias = dist_centroides[indices]  , indices = indices)
}
```

Ahora podemos llamar a la función `top_clustering_outliers` para obtener los índices y las distancias a los centroides de los outliers.

```{r}
top.outliers.kmeans = top_clustering_outliers(
  mis.datos.numericos.normalizados, 
  indices.clustering.bankloan, 
  centroides.normalizados.bankloan, 
  numero.de.outliers
)

cat("Índices de los top k clustering outliers (k-means, usando distancia euclídea)")
top.outliers.kmeans$indices 
cat("Distancias a sus centroides de los top k clustering outliers (k-means, usando distancia euclídea)")
top.outliers.kmeans$distancias
```

Podemos visualizar los outliers obtenidos con un BiPlot.

```{r}
numero.de.datos   = nrow(mis.datos.numericos)
is.kmeans.outlier = rep(FALSE, numero.de.datos) 
is.kmeans.outlier[top.outliers.kmeans$indices] = TRUE

BIPLOT.isOutlier             = is.kmeans.outlier
BIPLOT.cluster.colors        = c("blue","red","brown")
BIPLOT.asignaciones.clusters = indices.clustering.bankloan
MiBiPlot_Clustering_Outliers(mis.datos.numericos, "K-Means Clustering Outliers")
```

Como podemos comprobar, se han detectado únicamente outliers univariantes. Será necesario aumentar el valor de $k$ para encontrar outliers multivariados.

A continuación, vamos a obtener la posición real de los centroides, deshaciendo la normalización z-score. Es un proceso sencillo que consiste en despejar la fórmula $$\mathtt{z-score} = \frac{\mathtt{dato} - \mathtt{media.columna}}{\mathtt{sd.columna}}$$

```{r}
mis.datos.medias        = colMeans(mis.datos.numericos)
mis.datos.desviaciones  = apply(mis.datos.numericos, 2, sd , na.rm = TRUE)

centroides.valores      = sweep(centroides.normalizados.bankloan, 
                                2, 
                                mis.datos.desviaciones, 
                                "*")

centroides.valores      = sweep(centroides.valores, 
                                2, 
                                mis.datos.medias, 
                                "+")
```


### Clustering con PAM
Sería interesante utilizar otras técnicas de clustering para realizar este proceso de detección de anomalías.

A continuación aplicaremos clustering con PAM (Partition Around Medoids). Para ello, será necesario calcular la matriz de distancias de todos con todos usando la función `dist`.

```{r}
# Mostramos los valores normalizados y no normalizados de los índices
# Para ello, como los medoides son registros reales de nuestro conjunto de datos
# basta con acceder a sus valores. No tenemos que revertir el proceso de normalización
# como tuvimos que hacer con los centroides.


matriz.de.distancias              = dist(mis.datos.numericos.normalizados)
set.seed(2)
modelo.pam                        = pam(matriz.de.distancias , k = numero.de.clusters)
indices.asignacion.clustering.pam = modelo.pam$clustering
medoides.indices                  = modelo.pam$medoids
medoides.valores.normalizados     = mis.datos.numericos.normalizados[medoides.indices, ]
medoides.valores                  = mis.datos.numericos[medoides.indices, ]

top.outliers.pam = top_clustering_outliers(mis.datos.numericos.normalizados , 
                                           indices.asignacion.clustering.pam, 
                                           medoides.valores.normalizados, 
                                           numero.de.outliers)

cat("PAM Medoids\n")
print(medoides.valores)
cat("PAM Top Outliers\n")
print(top.outliers.pam$indices)
cat("kMeans Centroids\n")
print(centroides.valores)
cat("kMeans Top Outliers\n")
print(top.outliers.kmeans$indices)
```

Como podemos ver, la posición de los centroides y de los medoides es bastante distinta. Sin embargo, la mayor parte de los outliers seleccionados es la misma. Encontraríamos más diferencais con un valor más alto de $k$.

### Clustering con k-means, usando distancia de Mahalanobis
Podemos considerar la opción de usar una función de distancia distinta a la hora de calcular las distancias entre los puntos y los centroides o medoides. Por ejemplo, podríamos usar la distancia de Mahalanobis.

```{r}
top_clustering_outliers_distancia_mahalanobis = function(datos, 
                                                         indices.asignacion.clustering, 
                                                         numero.de.outliers){
  
  cluster.ids = unique(indices.asignacion.clustering)
  k           = length(cluster.ids)
  seleccion   = sapply(1:k, function(x) indices.asignacion.clustering == x)
  
  
  # Usando la estimación robusta de la media y covarianza
  lista.matriz.de.covarianzas   = lapply(1:k, function(x) cov.rob(mis.datos.numericos[seleccion[,x],])$cov)
  lista.vector.de.medias        = lapply(1:k, function(x) cov.rob(mis.datos.numericos[seleccion[,x],])$center)
  
  
  mah.distances   = lapply(1:k, 
                           function(x) mahalanobis(mis.datos.numericos[seleccion[,x],], 
                                                   lista.vector.de.medias[[x]], 
                                                   lista.matriz.de.covarianzas[[x]]))  
  
  todos.juntos = unlist(mah.distances)
  todos.juntos.ordenados = names(todos.juntos[order(todos.juntos, decreasing=TRUE)])
  indices.top.mah.outliers = as.numeric(todos.juntos.ordenados[1:numero.de.outliers])
  
  
  list(distancias = mah.distances[indices.top.mah.outliers]  , indices = indices.top.mah.outliers)
}

top.clustering.outliers.mah = top_clustering_outliers_distancia_mahalanobis(mis.datos.numericos, 
                                                                            indices.clustering.bankloan, 
                                                                            numero.de.outliers)

numero.de.datos = nrow(mis.datos.numericos)
is.kmeans.outlier.mah = rep(FALSE, numero.de.datos) 
is.kmeans.outlier.mah[top.clustering.outliers.mah$indices] = TRUE

BIPLOT.isOutlier             = is.kmeans.outlier.mah
BIPLOT.cluster.colors        = c("blue","red","brown")
MiBiPlot_Clustering_Outliers(mis.datos.numericos, "K-Means Clustering Outliers (Mahalanobis distance)")
```

Esta medida de distancia ofrece resultados muy distintos e interesantes, ya que se está teniendo en cuenta la dispersión dentro de cada cluster. Así pues, los outliers más evidentes ahora no son los datos más alejados, según la distancia euclídea, del cluster 1. En cambio, los 10 outliers más evidentes son aquellos datos del cluster 2 que claramente se separan considerablemente del núcleo, donde el resto de datos de este cluster están concentrados.

### Clustering con k-means, usando distancia relativa
A continuación vamos a usar la distancia relativa explicada en teoría como medida de distancia.
```{r}
###########################################################################
# Ampliación: 

# Definir la función top_clustering_outliers_distancia_relativa
# Esta función hará lo mismo que la función top_clustering_outliers
# pero usando como criterio la distancia relativa 
# (pag. 121 de las transparencias)

top_clustering_outliers_distancia_relativa = function(datos.normalizados, 
                                                      indices.asignacion.clustering, 
                                                      datos.centroides.normalizados, 
                                                      numero.de.outliers){
  
  dist_centroides = distancias_a_centroides(
    datos.normalizados, 
    indices.asignacion.clustering, 
    datos.centroides.normalizados
  )
  
  cluster.ids = unique(indices.asignacion.clustering)
  k           = length(cluster.ids)
  
  distancias.a.centroides.por.cluster    = sapply(
    1:k , 
    function(x) {
      dist_centroides[indices.asignacion.clustering == cluster.ids[x]]
    }
  )
  
  distancias.medianas.de.cada.cluster = sapply(
    1:k,
    function(x) {
      median(dist_centroides[[x]])
    }
  )
  
  todas.las.distancias.medianas.de.cada.cluster  =  
    distancias.medianas.de.cada.cluster[indices.asignacion.clustering]
  
  ratios = dist_centroides / todas.las.distancias.medianas.de.cada.cluster
  
  indices.top.outliers = order(ratios, decreasing=T)[1:numero.de.outliers]
  
  list(
    distancias = ratios[indices.top.outliers],
    indices = indices.top.outliers
  )
}



top.outliers.kmeans.distancia.relativa = top_clustering_outliers_distancia_relativa(
  mis.datos.numericos.normalizados,                                                                         indices.clustering.bankloan,                                                                              centroides.normalizados.bankloan,                                                                         numero.de.outliers
)


cat("Índices de los top k clustering outliers (k-means, usando distancia relativa)")
top.outliers.kmeans.distancia.relativa$indices 
cat("Distancias a sus centroides de los top k clustering outliers (k-means, usando distancia relativa)")
top.outliers.kmeans.distancia.relativa$distancias

is.kmeans.outlier.dist.rel = rep(F, nrow(mis.datos.numericos))
is.kmeans.outlier.dist.rel[top.outliers.kmeans.distancia.relativa$indices] = T

BIPLOT.isOutlier             = is.kmeans.outlier.dist.rel
BIPLOT.cluster.colors        = c("blue","red","brown")
MiBiPlot_Clustering_Outliers(mis.datos.numericos, "K-Means Clustering Outliers (relative distance)")
```

Como vemos, los resultados son distintos de nuevo. Esto pone de manifiesto la necesidad de contar con conocimiento experto a la hora de elegir un método para decidir qué son anomalías.



# Reglas de asociación

## Visualización de las distintas clases
En primer lugar, visualizaremos mejor los datos con unos diagramas de cajas con bigotes.

```{r}
myData = melt.data.frame(
  subset.numeric(dataset),
  id.vars = "impago"
)
ggplot(myData) +
  geom_boxplot(aes(impago, value, fill = impago)) +
  facet_wrap(~variable, scales = "free") +
  ggtitle("Distribución de frecuencias de cada variable, en función del impago")
```

Estos gráficos demuestran que las variables `morapred1`, `morapred2` y `morapred3`, correspondientes a las predicciones de morosidad tienen una relación muy fuerte con el impago.

## Discretización

La aplicación de las reglas de asociación requiere que todas las variables de entrada sean discretas. Por esto aplicaremos una discretización antes de comenzar con las reglas de asociación.

Este paso es crucial, y los valores elegidos aquí serán determinantes para el proceso que viene a continuación. Normalmente, un experto debería indicarnos cuáles son los mejores puntos de corte para la discretización, o bien se debería llevar a cabo un proceso iterativo buscando los puntos de corte que nos lleven a reglas más interesantes. En este caso, aplicaremos un método automático para la obtención de los puntos de corte que optimicen la separación de las clases.

```{r}
library(discretization)
dataset = cbind(subset(dataset, select = -impago), dataset[,"impago",drop = F])
myDataset.numeric = subset.numeric(dataset)
myDataset.rm.na = myDataset.numeric[1:700, ]
cutp = disc.Topdown(myDataset.rm.na)$cutp


dataset.discretized = myDataset.rm.na
for (i in 1:(ncol(myDataset.rm.na)-1)) {
  dataset.discretized[,i] = cut(myDataset.rm.na[,i], cutp[[i]], include.lowest = T)
}
dataset.discretized = cbind(dataset[1:700,"educ",drop=F], dataset.discretized)
```


## Obtención de las transacciones

Una vez realizada la discretización del conjunto de datos, procedemos a la tranformación en transacciones.

```{r}
library(arules)
transactions = as(dataset.discretized, "transactions")
```

Ahora podemos ver un resumen de las transacciones.

```{r}
summary(transactions)
# image(transactions)
```

## Itemsets frecuentes

Es conveniente comprobar con qué frecuencia aparece cada item. De este modo, podemos ver los items más relevantes.

```{r}
itemFrequencyPlot(transactions, support = 0.1, cex.names = 0.8)
```

Como podemos observar, los items más frecuentes son `deudaotro=(-Inf,12.8]` y `ingresos=(15.5, Inf]`, con un soporte de aproximadamente 0.98.

Ahora podemos buscar qué itemsets son más frecuentes. Para ello, usaremos el método _apriori_.

```{r}
iBankloan = apriori(transactions, parameter = list(support = 0.1, target="frequent"))
iBankloan = sort(iBankloan, by = "support")
inspect(head(iBankloan, 20))

myData = data.frame(itemset.size = as.factor(size(iBankloan)))
ggplot(myData) + 
  geom_bar(aes(itemset.size), fill = cbPalette[1]) +
  ggtitle("Frecuencias absolutas de los tamaños de todos los itemsets")
```


## Itemsets maximales y cerrados
A la vista de esta gráfica, comprobamos que los itemsets con 5 y 6 elementos son muy frecuentes. A continuación se mostrarán los 5 itemsets maximales con mayor soporte.

```{r}
imaxBankloan = iBankloan[is.maximal(iBankloan)]
inspect(head(sort(imaxBankloan, by = "support"), 5))

length(imaxBankloan)
myData = data.frame(itemset.size = as.factor(size(imaxBankloan)))
ggplot(myData) + 
  geom_bar(aes(itemset.size), fill = cbPalette[3]) +
  ggtitle("Frecuencias absolutas de los tamaño de los itemsets maximales")
```

Como podemos comprobar, los itemsets maximales no tienen un soporte demasiado elevado. También podemos contemplar los itemsets cerrados.

```{r}
icloBankloan = iBankloan[is.closed(iBankloan)]
inspect(head(sort(icloBankloan, by = "support"), 5))

length(icloBankloan)
myData = data.frame(itemset.size = as.factor(size(icloBankloan)))
ggplot(myData) + 
  geom_bar(aes(itemset.size), fill = cbPalette[2]) +
  ggtitle("Frecuencias absolutas de los tamaño de los itemsets cerrados")
```

A continuación se muestra un conteo de los itemsets de cada tipo.

```{r}
myData = data.frame(
  frequent = length(iBankloan),
  closed = length(icloBankloan),
  maximal = length(imaxBankloan)
)
myData = melt(myData)
ggplot(myData) + 
  geom_col(aes(variable, value, fill = variable)) + 
  scale_fill_manual(values=cbPalette) +
  ggtitle("Frecuencias absolutas del número de itemsets de distintos tipos")
```

## Extracción de reglas
Para obtener las reglas de asociación, podemos seguir el método apriori en una primera aproximación. El uso de este método es posible gracias a que el conjunto de datos es bastante pequeño y nos podemos permitir el lujo de usar una técnica computacionalmente muy costosa. En nuestro caso, buscaremos reglas con un soporte mínimo del 10% y una confianza mínima del 80%.

```{r}
rules = apriori(
  transactions, 
  parameter = list(
    support    = 0.1,
    confidence = 0.8,
    minlen     = 2
  )
)

summary(rules)
```

A continuación, ordenaremos las reglas por confianza, para mostrar las que mejor resultado producen con respecto a esta métrica.

```{r}
rules.byConfidence = sort(rules, by = "confidence")

inspect(head(rules.byConfidence))
quality(head(rules.byConfidence))
```

También podemos ordenarlas por lift, o cualquier otra métrica.

```{r}
rules.byLift = sort(rules, by = "lift")

inspect(head(rules.byLift))
quality(head(rules.byLift))
```

## Estudio de las reglas deseadas
Vamos a observar las reglas cuyo lift sea superior a 2 o contengan información sobre el impago en el consecuente. Así podremos explorar reglas que nos lleven a predecir el impago, así como otras reglas interesantes.

```{r}
mySelectedRules = subset(
  rules, 
  subset = rhs %in% c(
    "impago=Yes",
    "impago=No"
  ) |
  lift > 2 
)

inspect(head(mySelectedRules))
```

Podemos eliminar las reglas redundantes haciendo lo siguiente:

```{r}
subsetMatrix = is.subset(mySelectedRules, mySelectedRules)
subsetMatrix[lower.tri(subsetMatrix, diag = T)] = NA
redundant = colSums(subsetMatrix, na.rm = T) >= 1
rules.pruned = mySelectedRules[!redundant]

inspect(head(rules.pruned))
```


## Medidas de interés adicionales
Llegados a este punto, podemos decir que hemos encontrado regla que parecen muy interesante, ya que nos permiten predecir la morosidad a partir de otro parámetro. Sin embargo, será necesario estudiar algunas medidas adicionales para comprobar cómo de interesante pueden ser esta y otras reglas.

```{r}
myInterestMeasures = interestMeasure(
  rules.pruned, 
  measure = c(
    "hyperConfidence", 
    "leverage",
    "phi",
    "gini"
  ),
  transactions = transactions
)

quality(rules.pruned) = cbind(
  quality(rules.pruned), 
  myInterestMeasures
)

inspect(head(sort(rules.pruned, by = "gini")))
```

## Visualización
Ahora podemos visualizar las reglas obtenidas con el paquete `arulesViz`. Existen varios tipos de visualización, que nos dan distintas perspectivas sobre las reglas encontradas.

```{r}
library(arulesViz)

plot(rules)

plot(rules.pruned[1:11], method="graph", control=list(type="items"))

# plot(rules.pruned, method="grouped")
# 
# plot(
#   rules.pruned[1:10],
#   method="paracoord",
#   control = list(reorder = TRUE)
# )
```

Como podemos comprobar en el gráfico relacional, existen relaciones muy interesantes que son autoexplicativas. En concreto, parece que los predictores de morosidad son bastante efectivos, ya que existe una relación con un amplio soporte entre que los predictores tengan un valor bajo y que no se hayan producido impagos. Asímismo, hay una fuerte relación (com poco soporte, pero mucho lift) entre un valor alto de `morapred1` y que se produzcan impagos. Otra regla que puede ser de interés  es que tener un valor alto en la variable `empleo` parece estar relacionado con no incurrir a impago.

# Bibliografía
